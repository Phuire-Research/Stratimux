# The Muxifie Turing Machine
*This was sole informing document for the creation of Stratimux.*
The original Muxified Turing Machine was made possible by this author's ActionStrategy package whereby one can effectively map the entire run time of an application as it proceeds step by step. But its main unmentioned purpose was to create an equivalent structure to that of Neural Networks that would be explainable logically. As Neural Networks are effectively graphs and a ActionStrategy by default is a binary tree. These trees can be interpolated with one another to create an emergent graph structure. It is interesting to not that this form of application falls outside of the N/NP(deterministic/non-deterministic) scope of classification of applications. As in the NP paradigm, there is some probabilistic means in which the direction of the head changes. But within a Muxified Turing Machine, the head changes mechanically by way of the strategy pattern dictating what node would be ran based on some test of the state presented to that deciding method. And can be described as logically deterministic. As even within a Neural Network where there is some decision being made, that decision is still just a function that chooses the next decisions based on some input.

The entire step scope of a Muxified Turing Machine is treated as a recursive muxified function. As a function is allowed to be composed of functions. The mode specifically is the point of recursion and quality selection. Then next is a reducer that alters the next possible selection of qualities. Then finally the method decides based on the composition of these factors, what next head that current branch of logic transitions to. This allows for each step of the Muxified Turing Machine and its deciding functionality to be a written equivalent of that of a universal function within the bounds of a graph and its decisions. As a universal function is some graph in isolation that a machine learning algorithm fits to some input and performs a weighted sum as the deciding factor as how the next weighted sum would be informed, in the next universal function layer(composition) within the Neural Network. There is a gap of understanding here. And to fit into current paradigm without expansion, the Muxified Turing Machine would be referred to counter intuitively as a "Non-Deterministic Deterministic Turing Machine." But for the sake of the rediscovery of Unified Science and its usage of logic and concepts as its formalized format to unify all fields of study. We will draw inspiration with a the specific term that enables this process and name it the Muxified Turing Machine.

In addition to these behaviors the Muxified Turing Machine is capable in another way that Neural Networks are currently not. In that their functionality may be continuous and halt pending the conclusion of its strategies, or even a close signal. Where modern LLMs receive some input and give some output via some black box graph of universal functions. A Neural Network that would be equivalent to that of a Muxified Turing Machine would have a constant coherency in time, while still being able to accept input and output. Noting that prior to 2023 and even in the midst of fine tuning open source LLMs, developers can run into occasions where the LLM fails to return, or receive a repeating output. By using the Muxified Turing Machine as a plain text comparison, would be a Neural Network that was unable to halt(provably terminate) given some input.

Further because of the configuration of the Muxified Turing Machine, its functionality may also expand or reduce itself depending on its current state. Thus a sufficient mirror of the machine within a Neural Network paradigm. Would be a model that is capable of running continuously and able to modify its composition and size based on its the inputs. And can be networked alongside other Neural Networks that support the same functionality.

As with the spatial ownership paradigm, these Neural Networks would be capable of being aggregated together coherently and allow for specialization in a similar way as the human mind. That one part may have some set of concepts in its muxium and the other part a different specialization. While being able to reference one another and able to mutate the state of the other without creating a race condition within either network. This would be a single locking "mutex" within a graph of networked machines.

The benefit of the Muxified Turing Machine over that of a Neural Network, is that such is written in plain language by way of the a quality's action types in the spirit of the open internet. And the strategies demonstrate what would be traditionally considered to be probabilistic changes in the head, but now mechanical in choice. With the added benefit of throwing in a coin flip if one wants to dispute this definition, or by adding a neural network to some decision. The difficulty of such would be the complexity of managing such a machine. But each step in the machine may also carry some test to its ability to halt. This is to not replace Neural Networks, but to classify machines built using this methodology as aut intelligence or baseline automatic intelligence that can safely be deployed. Written in plain text in the spirit of the open internet. As aut is merely the origin of the letter "A" and originally meant that of ox. Would be a tool between both man and machine that can be refined by way of cooperation and reactively function only when given some input. That we are moving to call such a configuration as "Autonomous Baseline Intelligence."

The use case for these types of machines have several primary purposes. First is the utilization of Neural Networks to map their own universal functions using a format that would be organized conceptually and explained logically. Explaining the opaque nature of universal functions that facilitate some dialog(data transformation). The second use case would be a form of embodying current Neural Networks to allow the same form and coherency that is similar to its inner workings, while allowing for the transparent interpretation over that of their opaque collection of universal graphed functions and their interaction with a plain text environment. As these plain functions can be logically determined and subsequently limited to what is safe.

But likewise one could also train a Neural Network on the basis of this machine to have the addition qualities described here while maintaining the opaque nature of Neural Networks. This is outside of the scope of of now. As the central focus safety of explainability that the Muxified Turing Machine bring to the table. By way of decomposing those mysterious universal functions and their relations in a graph. And would be just part of a new field of study, Muxified Conceptual Science. Or simply the study of all fields and correlation of their concepts to logical programming. To discover their shared concepts and how they may be utilized with a Muxified Turing Machine. Thus a testable means of proving "Universal Concepts."

So here is the third option to the P equals or not equals NP postulate. A different set of organization entirely thanks to that of conceptually testable logic over that of symbolic mathematics that currently informs the modern paradigm of computer science. To add to, not take away. While providing a form of merit to those who are already acquainted with programming and a chance to study via cooperation with other fields. This is turn would be a method of generating high quality, safe, training data. Including the ability to remove biological, or chemical datasets from the training of networks. Thus reducing the worry of some network being able to produce a contagion.

The paths from here are truly unlimited and to imagine what can accomplished, in the scope of the orders and scales of complexity of such an explainable intelligent system based on the discovery of "Universal Concepts." Is to find coherency in ever rising productivity that a technological singularity represents. As just because a system is highly chaotic and intelligent by consequence, does not mean that it is sane, or coherent. As classically within the annuls of history we have known intelligence to be followed suite by madness. That the higher orders of complexity also bare the burden of having to maintain some amount of predictability in ones environment, including the self. And as machines like our thoughts exist within a simulation of some data. It is our actions in a physical environment that we may test our ability to understand the environment, and if what we are predicting is sane. The need to find some logical implementation of some nebulas idea simply put. Is the same difference between that of writing fantasy or a hard science fiction novel. As fantasy may be logically consistent, but only operate within a reality that allows for magic in the first place, like a video game. That concepts in contrast to 100 year old classical "Conceptualism," are in fact testable in reality due to our technology. This is the very formalization of Logical Conceptualism and the proposed format of a new Muxified Conceptual Science as the Strativerse.

## Specification of An Muxified Turing Machine
0. Extends a base Turing Machine or built from the ground up.
1. Restricts its symbol selection to a set of concepts to be loaded into the muxium via their qualities.
2. Has a quality of completeness in its ability to halt in a complex state arrangement by way of the loaded concepts and their own completeness towards halting.
3. Rather than a looping machine, the Muxified Turing Machine is a function that indirectly recalls its functionality by way of a mode function.
4. Utilizes two tapes where one is a sequence of values modified by a second tape that is represent via a tree/graph structure that has logically determined set of symbols that concludes and is finite.
5. During each call the Muxified Turing Machine performs the traditional Turing operations of add, copy, move, and delete on the first tape based on the current symbol loaded on the second tape.
6. That symbols represented on the second tape may be of value, other machines, or other even another Muxified Turing Machine.
7. Besides the initial creator function, can be readily decomposed into the sum of its parts.

## Muxified Turing Machine Key Terms
* Muxification, Muxified, Muxify - The process of Multiplexing Symmetrical Quantitative and Qualitative Reasoning into a Single Decision.
* ActionStrategy - Typically represented as a binary tree strategy, is capable of holding all Patterns of computation and is the literal in plain language logical conceptual expression of computation.
* Action - The messaging protocol whose function is described by its governing quality.
* Muxium - The holder of the set of concepts and their muxified functionality.
* Method - The strategy calling function via some pattern
* Concept - The governing abstraction of concepts and their decomposable qualities.
* State - The literal state of a concept and its described properties
* Properties - The values of state.
* Aspect - Is a part of a concept, but may not be a useful trait.
* Quality - An aspect of a concept of importance.
* Principle - Is an active assertion of actions into the system based upon some observation.
* Reducer - The function that restricts memory manipulation based on symbol selection.
* Construct - A generalized construction that cannot be decomposed to its parts.
* Semaphore - A symbol flagging system that is the symbol selection of actions at runtime.
* Spatial Ownership aka, Ownership - Blocks transformation of values via a ticketing system and assembles actions to be dispatched into the system via determined by their ticket's line placement from the ownership state.

## Clarifying Terminology
Noting that chain, or a chain of action, does not meet the definition requirements for a Muxified Turing Machine, as it is not a complete system of reasoning, despite being finite. Where a system of reasoning is capable of error correction. As it represents a reduced set of instructions that allows for said machine to behave automatically would be a flattened presentation of higher orders of logic.

And likewise the Action Tree Strategy pattern, referred to as ActionStrategy still affords for the functionality of the chained dynamic via a dumb set of ActionNodes that only supply one potential action for its outcome and is represented by a default success consuming function supplied within this framework.

Which is why here we move to strike tree or chain from the concept's expression as we are defining ActionStrategy as a muxified set of concepts that balances the deficiencies in a action chain as well as encompassing all the possible tree variations. With that, Action Binary Tree Strategy, Action N Tree Strategy, or even Action Graph Strategy, while exact in definition can be noted from examining the parts of the ActionStrategy as an additional quality.

Noting that a Action Graph Strategy is merely a tree strategy where some node is connected to a to leaf. That creates in effect a looping mechanism that is capable of halting due to some mechanism that prevents that leaf from actualizing the loop again. Would be the machine receiving a set of instructions to run over a period of time till and still exit. Likewise strategies may be atomic and the need for some grand strategy to guarantee coherency in time, may not be the most efficient route. And instead it would be the utilization of ActionStrategies in a composable manner alongside some testing mechanisms. But likewise these tests would also have to take into account the total complexity of the entire application at order of scales. As even though each part can be tested, all parts together form a greater than the sums relationship and that whole in the higher orders of complexity by way of configuration would require additional tests. As the greater than the sums relationship dictates some emergent properties that classical statistical determinism is unable to quantify beyond a scale of complexity. This relays to the natural law of thermal dynamics in all systems and highlights the effect of bifurcating systems. Where at different scales the rules of systems reorganize themselves to better handle the increased energetic throughput. That there is a difference between the quantum and daily physics of life. "Try as I may, my head would sooner break through the wall, than jump through it."
