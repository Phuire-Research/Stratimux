# Statement on Mechanistic Interpretability
"Unfortunately, it turns out that the individual neurons do not have consistent relationships to network behavior. For example, a single neuron in a small language model is active in many unrelated contexts, including: academic citations, English dialogue, HTTP requests, and Korean text." [Decomposing LLMS into Understandable Components](https://www.anthropic.com/index/decomposing-language-models-into-understandable-components)

The main concern and impact of this work directly coincides with the current direction in the scope of Ai Alignment. Where the current attempt is to move into the networks themselves to provide explainability and nuanced control of neurons or grouping of such. Using STRX as a method of comparison. It makes sense that neurons in a forward feed network style learn to maintain several different outputs on each neuron. In comparison these individual neurons can be seen as a "Axium." Where each have the capability of increasing the current "Weighted Sum," of that specific layer within the network.

The main difficulty if these two types of graphs have a comparable comparison, wherein each node is capable of multiple outputs that may be unlike. As cited above, may run into the same issue of hand written graph systems when effecting individual neurons. Where attempting to down regulate one neuron, may cause an side effect of another branch being unable to stop/halt. [Video Citation: The requirement to stop within a behavior tree. Artificial Intelligence Summit @GDC 2016 via Youtube](https://youtube.com/clip/UgkxtZlIbvaMv0OUCJ5kJFiaUCjmEQCBD0C6?si=tkrAkvbpqByq096U) Noting that this effect is present within fine tuned models, where one can likewise receive no output, or a repeating output.

As the difficulty being demonstrated within "Decomposing Large Language Models into Understandable Components." Demonstrates that there is some space saving mechanism being trained into networks. That allows for neurons to have such variable activation. That the entire scope of the network is likewise exponential in its scope, beyond the surface level comparison.

Therefore the main difficulty here would be effecting neural networks for the intended alignment, while also providing a guarantee that the specific regulation. Does not inadvertently cause a run away effect in other activations. Keeping in mind the exponential scope of that network.

## Statements
* [Safety Statement](https://github.com/Phuire-Research/STRX/blob/main/StatementSafety.md)
* [The Human Ability to Halt](https://github.com/Phuire-Research/STRX/blob/main/StatementHH.md)
* [Release Disclosure](https://github.com/Phuire-Research/STRX/blob/main/ReleaseDisclosure.md)